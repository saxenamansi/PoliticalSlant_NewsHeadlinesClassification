{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "062a19aa-c7a8-4e40-9d3e-28705abb3811",
   "metadata": {},
   "source": [
    "# LR Classification by Fine-Tuning DistilBERT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a1edf63-df09-4131-9b2c-1b27491c4bec",
   "metadata": {},
   "source": [
    "In this notebook, we Fine-Tune the DistilBERT model from the transformers library, for classifying News Headlines as Left-leaning or Right-leaning. The following functions are performed in this notebook:\n",
    "\n",
    "This link introduced the DistilBERT model: https://huggingface.co/docs/transformers/model_doc/distilbert\n",
    "\n",
    "It is essentially a \"distilled\" form of the BERT model, being 40% smaller in size, while retaining 97% of its language understanding abilities, thus being 60% faster.\n",
    "\n",
    "The labels are :\n",
    "\n",
    "- left-leaing: CNN, The Washington Post\n",
    "- Neutral: Business Insider, USA Today\n",
    "- Right-leaning: Fox News, Breitbart News\n",
    "\n",
    "We only use the publications that are Left and Right leaning.\n",
    "\n",
    "1. Importing necessary libraries\n",
    "2. Preparing the data for training\n",
    "3. Preparing the model for fine-tuning\n",
    "4. Defining helper functions and classes\n",
    "5. Fine-tuning the model\n",
    "6. Performing 10-fold Cross Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eea7205-952f-47bf-97e0-9ed518c0cfbc",
   "metadata": {},
   "source": [
    "## Importing necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bae371b9-00f2-4c09-8a59-b069967d2ba0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/msaxena4/3a_env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2023-11-18 18:01:30.439057: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-11-18 18:01:31.031760: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "import random\n",
    "import pickle as pkl\n",
    "\n",
    "# libraries to fine-tune the model on our data\n",
    "from transformers import BertTokenizer, AutoTokenizer\n",
    "from transformers import AutoTokenizer, TrainingArguments, Trainer\n",
    "from transformers import DistilBertForSequenceClassification, Trainer, TrainingArguments\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoModelForSequenceClassification, AdamW\n",
    "\n",
    "# libraries to evaluate our results\n",
    "from statsmodels.stats.contingency_tables import mcnemar\n",
    "from sklearn.metrics import f1_score, confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "de4c918a-22d7-49d4-9f00-d2991454b985",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting the random seed for Python's random module\n",
    "random_seed = 42\n",
    "random.seed(random_seed)\n",
    "\n",
    "# Setting the random seed for NumPy\n",
    "np.random.seed(random_seed)\n",
    "\n",
    "torch.manual_seed(random_seed)\n",
    "torch.cuda.manual_seed(random_seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False  # Disable cuDNN benchmark for reproducibility"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b0a5862-bc9c-4f6f-a110-b913be28441f",
   "metadata": {},
   "source": [
    "## Preparing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "721722a1-e70d-4895-b9b0-58b1fcd1cc2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(35886, 20)\n",
      "publication\n",
      "Fox News               8327\n",
      "Breitbart News         7377\n",
      "CNN                    6485\n",
      "Business Insider       4803\n",
      "The Washington Post    4678\n",
      "USA Today              4216\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Reading in the data\n",
    "news_df = pd.read_csv('../news_headlines_dataset.csv', index_col = 0)\n",
    "\n",
    "# Printing the shape of the data\n",
    "print(news_df.shape)\n",
    "\n",
    "# Selecting only the required features for fine-tuning\n",
    "news_df = news_df[['title', 'publication']]\n",
    "\n",
    "# printing the values in each label\n",
    "print(news_df.publication.value_counts())\n",
    "\n",
    "# creating the target column for classification\n",
    "# Left-leaning rows have value 0\n",
    "# Right-leaning rows have value 1\n",
    "# Neutral rows have value 2\n",
    "news_df['leaning'] = news_df.publication.str.replace('CNN', '0').str.replace('The Washington Post', '0').str.replace('Business Insider', '2').str.replace('USA Today', '2').str.replace('Fox News', '1').str.replace('Breitbart News', '1')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "31d2f460-8e48-4206-a052-645c047c63d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['title', 'publication', 'leaning'], dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# printing the columns\n",
    "news_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cdf34a80-7216-4aac-a965-2e89f221a1aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "leaning\n",
       "1    15704\n",
       "0    11163\n",
       "2     9019\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# printing the values in each label of the target column\n",
    "news_df['leaning'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b6c3f337-d8ab-4d6e-b693-17219584ac39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping the rows that are neutral, and are not left-leaning or right-leaning\n",
    "news_df = news_df[news_df['leaning'] != '2']\n",
    "\n",
    "# converting to int65 type\n",
    "news_df.leaning = news_df.leaning.astype('int64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "74cc1387-1c96-4e1f-89d9-751da0f11187",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['title', 'leaning'], dtype='object')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dropping the publication column\n",
    "news_df = news_df.drop(['publication'], axis=1)\n",
    "news_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "95d764e8-546c-4bba-b8a9-69a48d8e4659",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(news_df, target):\n",
    "    \"\"\"\n",
    "    This is the main text preprocessing function. It performs the following functions:\n",
    "    \n",
    "    1. Dropping duplicates\n",
    "    2. Undersampling the majority class samples\n",
    "    3. Random shuffling the dataset\n",
    "    \"\"\"\n",
    "    news_df = news_df.drop_duplicates()\n",
    "    first = news_df[news_df[target] == 1]\n",
    "    second = news_df[news_df[target] == 0]\n",
    "    \n",
    "    majority = first if len(first) > len(second) else second\n",
    "    minority = second if len(first) > len(second) else first\n",
    "\n",
    "    n = len(minority)\n",
    "    majority = majority.sample(n=n)\n",
    "\n",
    "    frames = [majority, minority]\n",
    "    result = pd.concat(frames).sample(frac=1, random_state=42)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "39231eac-7f17-49da-bc4f-f7ac4ed5a896",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# preprocessing the data\n",
    "result = preprocessing(news_df, \"leaning\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a9b88a04-00c3-4b4f-a325-b977dc1bd716",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(                                                   title  leaning\n",
       " 29176  Trump associates, including Giuliani, are aski...        0\n",
       " 21414  Donald Trump: Fox News No Longer Great, an Obs...        1\n",
       " 25432  William Cohen: Joe Biden will be a steady, sta...        1\n",
       " 19217  Sen. Brown: Putin Has Something on Trump  We H...        1\n",
       " 30901  Opinion: Neither alternative facts nor alterna...        0,\n",
       " (21350, 2),\n",
       " leaning\n",
       " 0    10675\n",
       " 1    10675\n",
       " Name: count, dtype: int64)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# a glance at the processed data\n",
    "result.head(), result.shape, result.leaning.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99421ae7-7be2-4acd-a991-46f27b3f751f",
   "metadata": {},
   "source": [
    "## Defining helper functions and classes for Fine-Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a4c48167-490b-4d1b-bf81-6b96505e6635",
   "metadata": {},
   "outputs": [],
   "source": [
    "class setDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"\n",
    "    This class defines the dataset with the text encodings of the \n",
    "    News Headlines and the label values \n",
    "    labels are 0 (left) or 1 (right)\n",
    "    \"\"\"\n",
    "    # initialising the dataset\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    # returning an item of the dataset\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    # returning the length of the dataset\n",
    "    def __len__(self):\n",
    "        return len(self.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f9c65a31-8d8e-44a5-899f-83e9bd4ff3ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pytorch_model(model, train_dataset):\n",
    "    \"\"\"\n",
    "    This function takes as input the model to be fine-tuned and the training dataset. \n",
    "    It utilisies GPU power to fine-tune the model, and then retrn the final model\n",
    "    \"\"\"\n",
    "\n",
    "    device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "    \n",
    "    model.to(device)\n",
    "    model.train()\n",
    "\n",
    "    # loading the data\n",
    "    train_loader = DataLoader(train_dataset, batch_size=8, shuffle=False)\n",
    "\n",
    "    # initialising the optimiser\n",
    "    optim = AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "    # training the model\n",
    "    for epoch in range(3):\n",
    "        for batch in train_loader:\n",
    "            optim.zero_grad()\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            loss = outputs[0]\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "    model.eval()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "857a06f9-df8a-4e4e-a563-0d69a28ab599",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions(X_test, tokenizer, model):\n",
    "    \"\"\"\n",
    "    This function gets the predictons of the test set from the fine-tuned model.\n",
    "    It used a batch size of a 100.\n",
    "    It takes as input the test set, the tokeniser and the model.\n",
    "    It gets a batch of data from the test set, tokenises it, and then predicts the labels.\n",
    "    After getting labels for all batches, it compiles the results together and returns it.\n",
    "    \"\"\"\n",
    "    device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "    \n",
    "    x = 0\n",
    "    pred_labs = []\n",
    "    while x < X_test.shape[0]:\n",
    "        if((x+10) < X_test.shape[0]):\n",
    "            y = x + 10\n",
    "        else:\n",
    "            y = X_test.shape[0]\n",
    "\n",
    "        x_test = X_test[x:y].tolist()\n",
    "        inputs = tokenizer(x_test,  padding=True, truncation=True, max_length=500, return_tensors=\"pt\").to(device)\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "        y_pred = [int(x.argmax()) for x in outputs[0].softmax(1)]\n",
    "        pred_labs.extend(y_pred)\n",
    "        x+=10\n",
    "        \n",
    "    return pred_labs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ee110a07-c465-4146-87c6-76dd161fd149",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(data):\n",
    "    \n",
    "    \"\"\"\n",
    "    This is the main training function that combined the \n",
    "    functionalities discussed above - \n",
    "\n",
    "    1. Initialising the tokeniser and the model\n",
    "    2. Encoding the training and testing datasets using the tokeniser\n",
    "    3. Creating an instance of the Dataset class for the training and testing encoding\n",
    "    4. Fine-tuning the DistilBERT model\n",
    "    5. Getting predictions for the test set using the newly fine-tuned model\n",
    "    6. Evaluating the results.\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    # initialising the tokeniser and the model for fine-tuning\n",
    "    # the model is taken from the transformer's library\n",
    "    tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased', use_fast=True)\n",
    "    autoBERTmodel = AutoModelForSequenceClassification.from_pretrained('bert-base-uncased')\n",
    "\n",
    "    device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "    X_train, X_test, y_train, y_test = data\n",
    "\n",
    "    # getting train and test set encodings\n",
    "    train_encodings = tokenizer(X_train.tolist(), truncation=True, max_length=500, padding=True)\n",
    "    test_encodings = tokenizer(X_test.tolist(), truncation=True, max_length=500, padding=True)\n",
    "\n",
    "    # getting train dataset and test dataset\n",
    "    train_dataset = setDataset(train_encodings, y_train)\n",
    "    test_dataset = setDataset(test_encodings, y_test)\n",
    "\n",
    "    # fine-tuning and predicting the results\n",
    "    ftBERTmodel = get_pytorch_model(autoBERTmodel, train_dataset)   # training the model\n",
    "    y_pred = get_predictions(X_test, tokenizer, ftBERTmodel)      # test set predictions\n",
    "    train_pred = get_predictions(X_train, tokenizer, ftBERTmodel)   # train set predictions\n",
    "\n",
    "    # Evaluation metrics\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    \n",
    "    test_acc = accuracy_score(y_test, y_pred)  # testset accuracies\n",
    "    train_acc = accuracy_score(y_train, train_pred)  # trainset accuracies\n",
    "    test_f1_score = f1_score(y_test, y_pred)  # testset f1 score\n",
    "    train_f1_score = f1_score(y_train, train_pred)  # train set f1 score\n",
    "\n",
    "    return test_acc, train_acc, test_f1_score, train_f1_score, y_pred, train_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bc1764a-9788-4f1d-be53-1d141d9fe52a",
   "metadata": {},
   "source": [
    "## Model Fine-Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "91accede-ecfa-41aa-8965-d1a2e5fc78ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the text data and the labels for model fine-tuning\n",
    "X = np.array(result.title)\n",
    "y = np.array(result.leaning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9364ca0e-ee74-46ec-99a1-4b705834116c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.82      0.85      2135\n",
      "           1       0.83      0.89      0.86      2135\n",
      "\n",
      "    accuracy                           0.85      4270\n",
      "   macro avg       0.85      0.85      0.85      4270\n",
      "weighted avg       0.85      0.85      0.85      4270\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# splitting the data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                    stratify=y, \n",
    "                                                    test_size=0.2, \n",
    "                                                    random_state = 42)\n",
    "data = (X_train, X_test, y_train, y_test)\n",
    "\n",
    "# calling the main fine-tuning function and \n",
    "# getting the evaluation results of the test set\n",
    "test_acc, train_acc, test_f1_score, train_f1_score, y_pred, train_pred = training(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ca6ec92-b9fc-4ec7-bf94-d72e5340cfd1",
   "metadata": {},
   "source": [
    "## Performing 10-fold Cross Validation\n",
    "\n",
    "Cross-validation is a resampling method that uses different portions of the data to test and train a model on different iterations.\n",
    "\n",
    "In this case, we split the data into 10 parts, and for each iteration, we use one part for testing and the remaining nine for training. \n",
    "\n",
    "If the evaluation results are similar across all folds, our model is robust"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c080772e-eecf-4a9a-bae3-aeae736d7649",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialising lists to store the evaluation results\n",
    "\n",
    "train_predictions = []\n",
    "test_predictions = []\n",
    "test_accs = []\n",
    "train_accs = []\n",
    "test_f1 = []\n",
    "train_f1 = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d4a891e8-abae-4a53-9fc2-48aa279cda2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((17080,), (4270,))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6124c7e2-ea88-47d0-9333-e335b5a530dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((17080,), (4270,))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9e1d63c8-7401-422f-afd9-ca06d390438a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold:  0 \n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.92      0.86      1068\n",
      "           1       0.90      0.78      0.84      1067\n",
      "\n",
      "    accuracy                           0.85      2135\n",
      "   macro avg       0.86      0.85      0.85      2135\n",
      "weighted avg       0.86      0.85      0.85      2135\n",
      "\n",
      "Fold:  1 \n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.92      0.85      1068\n",
      "           1       0.91      0.74      0.82      1067\n",
      "\n",
      "    accuracy                           0.83      2135\n",
      "   macro avg       0.85      0.83      0.83      2135\n",
      "weighted avg       0.85      0.83      0.83      2135\n",
      "\n",
      "Fold:  2 \n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.88      0.87      1068\n",
      "           1       0.88      0.84      0.86      1067\n",
      "\n",
      "    accuracy                           0.86      2135\n",
      "   macro avg       0.86      0.86      0.86      2135\n",
      "weighted avg       0.86      0.86      0.86      2135\n",
      "\n",
      "Fold:  3 \n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.93      0.86      1068\n",
      "           1       0.91      0.78      0.84      1067\n",
      "\n",
      "    accuracy                           0.85      2135\n",
      "   macro avg       0.86      0.85      0.85      2135\n",
      "weighted avg       0.86      0.85      0.85      2135\n",
      "\n",
      "Fold:  4 \n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.92      0.85      1068\n",
      "           1       0.91      0.76      0.83      1067\n",
      "\n",
      "    accuracy                           0.84      2135\n",
      "   macro avg       0.85      0.84      0.84      2135\n",
      "weighted avg       0.85      0.84      0.84      2135\n",
      "\n",
      "Fold:  5 \n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.92      0.86      1067\n",
      "           1       0.91      0.76      0.83      1068\n",
      "\n",
      "    accuracy                           0.84      2135\n",
      "   macro avg       0.85      0.84      0.84      2135\n",
      "weighted avg       0.85      0.84      0.84      2135\n",
      "\n",
      "Fold:  6 \n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.91      0.84      1067\n",
      "           1       0.90      0.75      0.82      1068\n",
      "\n",
      "    accuracy                           0.83      2135\n",
      "   macro avg       0.84      0.83      0.83      2135\n",
      "weighted avg       0.84      0.83      0.83      2135\n",
      "\n",
      "Fold:  7 \n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.87      0.85      1067\n",
      "           1       0.87      0.81      0.84      1068\n",
      "\n",
      "    accuracy                           0.84      2135\n",
      "   macro avg       0.84      0.84      0.84      2135\n",
      "weighted avg       0.84      0.84      0.84      2135\n",
      "\n",
      "Fold:  8 \n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.76      0.81      1067\n",
      "           1       0.79      0.87      0.83      1068\n",
      "\n",
      "    accuracy                           0.82      2135\n",
      "   macro avg       0.82      0.82      0.82      2135\n",
      "weighted avg       0.82      0.82      0.82      2135\n",
      "\n",
      "Fold:  9 \n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.85      0.85      1067\n",
      "           1       0.85      0.84      0.85      1068\n",
      "\n",
      "    accuracy                           0.85      2135\n",
      "   macro avg       0.85      0.85      0.85      2135\n",
      "weighted avg       0.85      0.85      0.85      2135\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "1. Performing 10-fold cross validation by calling the training function \n",
    "on each split of the data.\n",
    "2. The split was made earlier (using the same preprocessing methods discussed above) \n",
    "3. The saved splits are simply imported here. \n",
    "\"\"\"\n",
    "\n",
    "for fold in range(10):\n",
    "    print(\"Fold: \", fold, \"\\n\\n\")\n",
    "\n",
    "    # importing the data\n",
    "    with open(f\"../LR_tenfoldCV/fold_{fold}_train_data.pkl\", \"rb\") as f:\n",
    "        X_train, y_train = pkl.load(f)\n",
    "\n",
    "    with open(f\"../LR_tenfoldCV/fold_{fold}_test_data.pkl\", \"rb\") as f:\n",
    "        X_test, y_test = pkl.load(f)\n",
    "    \n",
    "    data = (X_train, X_test, y_train, y_test)\n",
    "\n",
    "    # model training \n",
    "    test_acc, train_acc, test_f1_score, train_f1_score, y_pred, train_pred = training(data)\n",
    "\n",
    "    # compiling evaluation metrics\n",
    "    test_accs.append(test_acc)\n",
    "    train_accs.append(train_acc)\n",
    "    test_f1.append(test_f1_score)\n",
    "    train_f1.append(train_f1_score)\n",
    "    test_predictions.append(y_pred)\n",
    "    train_predictions.append(train_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee77c719-5c26-42dc-99a2-413c5f896c14",
   "metadata": {},
   "source": [
    "Since the results are similar, we prove that our model is robust, and the accuracy of our trained model is 85%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d56682d-2718-4f51-b5a2-9cfb8ec61b59",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3a_env",
   "language": "python",
   "name": "3a_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
